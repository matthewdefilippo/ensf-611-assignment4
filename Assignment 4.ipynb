{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 4: Pipelines and Hyperparameter Tuning (32 total marks)\n",
    "### Due: November 22 at 11:59pm\n",
    "\n",
    "### Name: Matthew De Filippo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will be putting together everything you have learned so far. You will need to find your own dataset, do all the appropriate preprocessing, test different supervised learning models and evaluate the results. More details for each step can be found below.\n",
    "\n",
    "### You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf275ca7",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b67a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219f163",
   "metadata": {},
   "source": [
    "## Step 1: Data Input (4 marks)\n",
    "\n",
    "Import the dataset you will be using. You can download the dataset onto your computer and read it in using pandas, or download it directly from the website. Answer the questions below about the dataset you selected. \n",
    "\n",
    "To find a dataset, you can use the resources listed in the notes. The dataset can be numerical, categorical, text-based or mixed. If you want help finding a particular dataset related to your interests, please email the instructor.\n",
    "\n",
    "**You cannot use a dataset that was used for a previous assignment or in class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2af8bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset (1 mark)\n",
    "df = pd.read_csv('imdb_top_1000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20316765",
   "metadata": {},
   "source": [
    "### Questions (3 marks)\n",
    "\n",
    "1. (1 mark) What is the source of your dataset?\n",
    "1. (1 mark) Why did you pick this particular dataset?\n",
    "1. (1 mark) Was there anything challenging about finding a dataset that you wanted to use?\n",
    "\n",
    "#### Question 1 Response\n",
    "The source of my dataset is Kaggle. It can be found at the following link:\n",
    "https://www.kaggle.com/datasets/harshitshankhdhar/imdb-dataset-of-top-1000-movies-and-tv-shows\n",
    "\n",
    "#### Question 2 Response\n",
    "I selected this dataset because I am a fan of movies and thought it would be interesting to develop a regression model that\n",
    "could predict the IMDB rating (i.e. aggregated user score) based on several attributes such as meta score (i.e. aggregated critic rating, run time, release year, number of votes, gross, and certificate (i.e. PG, G, PG-13, etc).\n",
    "\n",
    "#### Question 3 Response\n",
    "Yes; I wanted to find a dataset that was interesting so I spent a decent amount of time browsing Kaggle. There were some datasets that seemed to be poorly put together that I had to filter through. I found that using the usability filter on Kaggle allowed me to browse through higher quality datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea4cc",
   "metadata": {},
   "source": [
    "## Step 2: Data Processing (5 marks)\n",
    "\n",
    "The next step is to process your data. Implement the following steps as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afc244d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Poster_Link</th>\n",
       "      <th>Series_Title</th>\n",
       "      <th>Released_Year</th>\n",
       "      <th>Certificate</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Genre</th>\n",
       "      <th>IMDB_Rating</th>\n",
       "      <th>Overview</th>\n",
       "      <th>Meta_score</th>\n",
       "      <th>Director</th>\n",
       "      <th>Star1</th>\n",
       "      <th>Star2</th>\n",
       "      <th>Star3</th>\n",
       "      <th>Star4</th>\n",
       "      <th>No_of_Votes</th>\n",
       "      <th>Gross</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://m.media-amazon.com/images/M/MV5BMDFkYT...</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>1994</td>\n",
       "      <td>A</td>\n",
       "      <td>142 min</td>\n",
       "      <td>Drama</td>\n",
       "      <td>9.3</td>\n",
       "      <td>Two imprisoned men bond over a number of years...</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Frank Darabont</td>\n",
       "      <td>Tim Robbins</td>\n",
       "      <td>Morgan Freeman</td>\n",
       "      <td>Bob Gunton</td>\n",
       "      <td>William Sadler</td>\n",
       "      <td>2343110</td>\n",
       "      <td>28,341,469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://m.media-amazon.com/images/M/MV5BM2MyNj...</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>1972</td>\n",
       "      <td>A</td>\n",
       "      <td>175 min</td>\n",
       "      <td>Crime, Drama</td>\n",
       "      <td>9.2</td>\n",
       "      <td>An organized crime dynasty's aging patriarch t...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>Marlon Brando</td>\n",
       "      <td>Al Pacino</td>\n",
       "      <td>James Caan</td>\n",
       "      <td>Diane Keaton</td>\n",
       "      <td>1620367</td>\n",
       "      <td>134,966,411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://m.media-amazon.com/images/M/MV5BMTMxNT...</td>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>2008</td>\n",
       "      <td>UA</td>\n",
       "      <td>152 min</td>\n",
       "      <td>Action, Crime, Drama</td>\n",
       "      <td>9.0</td>\n",
       "      <td>When the menace known as the Joker wreaks havo...</td>\n",
       "      <td>84.0</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>Christian Bale</td>\n",
       "      <td>Heath Ledger</td>\n",
       "      <td>Aaron Eckhart</td>\n",
       "      <td>Michael Caine</td>\n",
       "      <td>2303232</td>\n",
       "      <td>534,858,444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Poster_Link  \\\n",
       "0  https://m.media-amazon.com/images/M/MV5BMDFkYT...   \n",
       "1  https://m.media-amazon.com/images/M/MV5BM2MyNj...   \n",
       "2  https://m.media-amazon.com/images/M/MV5BMTMxNT...   \n",
       "\n",
       "               Series_Title  Released_Year Certificate  Runtime  \\\n",
       "0  The Shawshank Redemption           1994           A  142 min   \n",
       "1             The Godfather           1972           A  175 min   \n",
       "2           The Dark Knight           2008          UA  152 min   \n",
       "\n",
       "                  Genre  IMDB_Rating  \\\n",
       "0                 Drama          9.3   \n",
       "1          Crime, Drama          9.2   \n",
       "2  Action, Crime, Drama          9.0   \n",
       "\n",
       "                                            Overview  Meta_score  \\\n",
       "0  Two imprisoned men bond over a number of years...        80.0   \n",
       "1  An organized crime dynasty's aging patriarch t...       100.0   \n",
       "2  When the menace known as the Joker wreaks havo...        84.0   \n",
       "\n",
       "               Director           Star1           Star2          Star3  \\\n",
       "0        Frank Darabont     Tim Robbins  Morgan Freeman     Bob Gunton   \n",
       "1  Francis Ford Coppola   Marlon Brando       Al Pacino     James Caan   \n",
       "2     Christopher Nolan  Christian Bale    Heath Ledger  Aaron Eckhart   \n",
       "\n",
       "            Star4  No_of_Votes        Gross  \n",
       "0  William Sadler      2343110   28,341,469  \n",
       "1    Diane Keaton      1620367  134,966,411  \n",
       "2   Michael Caine      2303232  534,858,444  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean data (if needed)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fc1ed40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Released_Year</th>\n",
       "      <th>Certificate</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>IMDB_Rating</th>\n",
       "      <th>Meta_score</th>\n",
       "      <th>No_of_Votes</th>\n",
       "      <th>Gross</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1994</td>\n",
       "      <td>A</td>\n",
       "      <td>142 min</td>\n",
       "      <td>9.3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2343110</td>\n",
       "      <td>28,341,469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1972</td>\n",
       "      <td>A</td>\n",
       "      <td>175 min</td>\n",
       "      <td>9.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1620367</td>\n",
       "      <td>134,966,411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008</td>\n",
       "      <td>UA</td>\n",
       "      <td>152 min</td>\n",
       "      <td>9.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2303232</td>\n",
       "      <td>534,858,444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1974</td>\n",
       "      <td>A</td>\n",
       "      <td>202 min</td>\n",
       "      <td>9.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1129952</td>\n",
       "      <td>57,300,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1957</td>\n",
       "      <td>U</td>\n",
       "      <td>96 min</td>\n",
       "      <td>9.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>689845</td>\n",
       "      <td>4,360,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Released_Year Certificate  Runtime  IMDB_Rating  Meta_score  No_of_Votes  \\\n",
       "0           1994           A  142 min          9.3        80.0      2343110   \n",
       "1           1972           A  175 min          9.2       100.0      1620367   \n",
       "2           2008          UA  152 min          9.0        84.0      2303232   \n",
       "3           1974           A  202 min          9.0        90.0      1129952   \n",
       "4           1957           U   96 min          9.0        96.0       689845   \n",
       "\n",
       "         Gross  \n",
       "0   28,341,469  \n",
       "1  134,966,411  \n",
       "2  534,858,444  \n",
       "3   57,300,000  \n",
       "4    4,360,000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, I want to get rid of some attributes that I know I will not need for my analysis.\n",
    "df = df.drop(columns = ['Poster_Link', 'Series_Title', 'Genre', 'Overview', 'Director', 'Star1', 'Star2', 'Star3', 'Star4'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59771b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Released_Year      0\n",
       "Certificate      101\n",
       "Runtime            0\n",
       "IMDB_Rating        0\n",
       "Meta_score       157\n",
       "No_of_Votes        0\n",
       "Gross            169\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, let's take a look to see if there are any null values.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "824a7678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we see that Certificate, Meta_score, and Gross contain over 100 null values. We will need to deal with these.\n",
    "# Since it is unclear what these values should be and guessing a value will impact the integrity of the data, I am simply \n",
    "# going to drop all null values from the dataset.\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ac0a8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Released_Year      int64\n",
       "Certificate       object\n",
       "Runtime           object\n",
       "IMDB_Rating      float64\n",
       "Meta_score       float64\n",
       "No_of_Votes        int64\n",
       "Gross             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the datatypes that we have.\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1decc612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Released_Year      int32\n",
       "Certificate       object\n",
       "Runtime            int32\n",
       "IMDB_Rating      float64\n",
       "Meta_score       float64\n",
       "No_of_Votes        int64\n",
       "Gross              int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice that Runtime is an object datatype. This is because it is stored as a string with the word \"min\" concatenated with it.\n",
    "# Let's convert this to an integer so we can perform proper regression. We also need to update the 'Released_Year' and 'Gross' to\n",
    "# integer variables as well.\n",
    "df['Runtime'] = df['Runtime'].str.extract('(\\d+)').astype(int)\n",
    "df['Released_Year'] = df['Released_Year'].astype(int)\n",
    "df['Gross'] = df['Gross'].str.replace(',', '').astype(int)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70a8c127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement preprocessing steps. Remember to use ColumnTransformer if more than one preprocessing method is needed.\n",
    "# The only preprocessing we are going to need here is to encode the movie certificates.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(sparse_output=False)\n",
    "enc.fit(df[['Certificate']])\n",
    "encoded_df = pd.DataFrame(enc.transform(df[['Certificate']]), columns=enc.get_feature_names_out())\n",
    "df = pd.concat([df, encoded_df], axis=1).drop(columns=['Certificate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b8e068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()\n",
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92c46b7",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "\n",
    "1. (1 mark) Were there any missing/null values in your dataset? If yes, how did you replace them and why? If no, describe how you would've replaced them and why.\n",
    "2. (1 mark) What type of data do you have? What preprocessing methods would you have to apply based on your data types?\n",
    "\n",
    "#### Question 1\n",
    "Yes. There were several missing/null values in my dataset. As per the above comments, I decided to remove all rows with null values in order to maintain the integrity of the dataset. I would not know what values to replace these with.\n",
    "\n",
    "#### Question 2\n",
    "I have a mixture of numerical data (Released_Year, Runtime, IMDB_Rating, Meta_score, No_of_Votes, Gross) and categorical data (Certificate). Since I have categorical data, I had to use one-hot encoding on the Certificate attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a245d00",
   "metadata": {},
   "source": [
    "## Step 3: Implement Machine Learning Model (11 marks)\n",
    "\n",
    "In this section, you will implement three different supervised learning models (one linear and two non-linear) of your choice. You will use a pipeline to help you decide which model and hyperparameters work best. It is up to you to select what models to use and what hyperparameters to test. You can use the class examples for guidance. You must print out the best model parameters and results after the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5558a776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement pipeline and grid search here. Can add more code blocks if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "748f23f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split the data into our training and validation sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop(columns = 'IMDB_Rating')\n",
    "y = df['IMDB_Rating']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bae54107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "\n",
    "# Linear Regression Pipeline\n",
    "lr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LinearRegression())\n",
    "])\n",
    "\n",
    "# Random Forest Pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestRegressor())\n",
    "])\n",
    "\n",
    "# GBR Pipeline\n",
    "gbr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', GradientBoostingRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "592691c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grids.\n",
    "param_grid_lr = {} \n",
    "\n",
    "\n",
    "param_grid_rf = {\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__max_depth': [10, 20, None]\n",
    "}\n",
    "\n",
    "param_grid_gbr = {\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'classifier__max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Define the scoring metrics.\n",
    "scoring = {\n",
    "    'mean_squared_error': make_scorer(mean_squared_error),\n",
    "    'r2_score': make_scorer(r2_score)\n",
    "}\n",
    "\n",
    "# Create GridSearchCV instances for each algorithm with multiple scoring metrics\n",
    "grid_search_lr = GridSearchCV(lr_pipeline, param_grid_lr, cv=5, scoring=scoring, refit='r2_score')\n",
    "grid_search_rf = GridSearchCV(rf_pipeline, param_grid_rf, cv=5, scoring=scoring, refit='r2_score')\n",
    "grid_search_gbr = GridSearchCV(gbr_pipeline, param_grid_gbr, cv=5, scoring=scoring, refit='r2_score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b923c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the models\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "grid_search_gbr.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters based on F1\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "best_params_lr = grid_search_lr.best_params_\n",
    "best_params_gbr = grid_search_gbr.best_params_\n",
    "\n",
    "# Access the results for both scoring metrics\n",
    "results_rf = grid_search_rf.cv_results_\n",
    "results_lr = grid_search_lr.cv_results_\n",
    "results_gbr = grid_search_gbr.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a502aaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression Results:\n",
      "R2 scores: [0.54047612]\n",
      "MSE scores: [0.03066882]\n",
      "Best Parameters for Logistic Regression based on R2 Score: {} \n",
      "\n",
      "Random Forest Results:\n",
      "R2 scores: [0.60985009 0.60889271 0.61278754 0.61113792 0.60430742 0.61697171]\n",
      "MSE scores: [0.02627654 0.02631146 0.02602027 0.02609873 0.02659183 0.02572411]\n",
      "Best Parameters for Random Forest based on R2 Score: {'classifier__max_depth': None, 'classifier__n_estimators': 200} \n",
      "\n",
      "\n",
      "Gradient Boosted Regressor Results:\n",
      "R2 scores: [0.40629032 0.51440837 0.46805492 0.55617581 0.49260742 0.55194381\n",
      " 0.5768896  0.55748626 0.57907432 0.55965261 0.55613517 0.53853788\n",
      " 0.5511261  0.53996379 0.54491047 0.54852363 0.53010136 0.56047557]\n",
      "MSE scores: [0.03965134 0.03239641 0.03540282 0.02956137 0.03393822 0.03004674\n",
      " 0.02819713 0.02946402 0.02793037 0.029177   0.02947279 0.03086858\n",
      " 0.02978851 0.03059196 0.03046288 0.02995356 0.03141106 0.02931356]\n",
      "\n",
      "Best Parameters for GBR based on R2 Score: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 5, 'classifier__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Print the results for r2 score and mean squared error.\n",
    "print(\"\\nLinear Regression Results:\")\n",
    "print(\"R2 scores:\", results_lr['mean_test_r2_score'])\n",
    "print(\"MSE scores:\", results_lr['mean_test_mean_squared_error'])\n",
    "print(\"Best Parameters for Logistic Regression based on R2 Score:\", best_params_lr, \"\\n\")\n",
    "\n",
    "print(\"Random Forest Results:\")\n",
    "print(\"R2 scores:\", results_rf['mean_test_r2_score'])\n",
    "print(\"MSE scores:\", results_rf['mean_test_mean_squared_error'])\n",
    "print(\"Best Parameters for Random Forest based on R2 Score:\", best_params_rf, \"\\n\")\n",
    "\n",
    "print(\"\\nGradient Boosted Regressor Results:\")\n",
    "print(\"R2 scores:\", results_gbr['mean_test_r2_score'])\n",
    "print(\"MSE scores:\", results_gbr['mean_test_mean_squared_error'])\n",
    "print(\"\\nBest Parameters for GBR based on R2 Score:\", best_params_gbr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd7075",
   "metadata": {},
   "source": [
    "### Questions (5 marks)\n",
    "\n",
    "1. (1 mark) Do you need regression or classification models for your dataset?\n",
    "1. (2 marks) Which models did you select for testing and why?\n",
    "1. (2 marks) Which model worked the best? Does this make sense based on the theory discussed in the course and the context of your dataset?\n",
    "\n",
    "#### Question 1 Response\n",
    "I need regression model for my dataset because my target variable, 'IMDB_Rating' is a continuous numberic variable. I am trying to predict the value of this target variable using the remaining features.\n",
    "\n",
    "#### Question 2 Response\n",
    "I selected the following three (3) models: LinearRegression, RandomForestRegressor, and GradientBoostingRegressor. I selected these models because they represent a range of different model complexities. I thought it would be interesting to see how these models perform from the most straightforward to implement (LinearRegression) to the most complicated (GradientBoostingRegressor).\n",
    "\n",
    "#### Question 3 Response\n",
    "Based on the R2 scores shown above, the best results were achieved with the RandomForestRegressor model. The maximum R2 score of 0.62 was achieved using this model. This does make sense based on the theory discussed in class. It is a more sophisticated model than LinearRegression. While the GradientBoostedRegressor can theoretically offer better results, this requires careful parameter tuning; perhaps more parameter tuning would be required to outperform the RandomForestRegressor model in this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f994e31",
   "metadata": {},
   "source": [
    "## Step 4: Validate Model (6 marks)\n",
    "\n",
    "Use the testing set to calculate the testing accuracy for the best model determined in Step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "69e64c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy (R2 Score) = 0.7852414913970109\n",
      "Validation Accuracy (R2 Score) = 0.5635404375177003\n"
     ]
    }
   ],
   "source": [
    "# Calculate testing accuracy (1 mark)\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Instantiate the Random Forest Classifier model with the parameters found in step 3.\n",
    "model = RandomForestRegressor(max_depth=5, n_estimators=200)\n",
    "model.fit(X_train, y_train)\n",
    "scores = cross_validate(model, X_train, y_train, cv=5, scoring='r2', return_train_score=True)\n",
    "print(f\"Training Accuracy (R2 Score) = {scores['train_score'].mean()}\" )\n",
    "print(f\"Validation Accuracy (R2 Score) = {scores['test_score'].mean()}\" )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4529ba",
   "metadata": {},
   "source": [
    "\n",
    "### Questions (5 marks)\n",
    "\n",
    "1. (1 mark) Which accuracy metric did you choose? \n",
    "1. (1 mark) How do these results compare to those in part 3? Did this model generalize well?\n",
    "1. (3 marks) Based on your results and the context of your dataset, did the best model perform \"well enough\" to be used out in the real-world? Why or why not? Do you have any suggestions for how you could improve this analysis?\n",
    "\n",
    "#### Question 1 Response\n",
    "I used R2 score as the accuracy metric so I could adequately compare the results to that achieved in the previous section.\n",
    "\n",
    "#### Question 2 Response\n",
    "An R2 training score of 0.79 was achieved; this surpassed the score of 0.62 achieved in the previous section. As such, we can say that this model did generalize well.\n",
    "\n",
    "#### Question 3 Response\n",
    "No; this model did not perform well enough to be used in the real-world. The model is exhibiting high variance; we are overfitting the data. Perhaps the data is just not that well correlated or the model requires further tuning. Some potential changes we could make to the model include:\n",
    "- Further tuning of the RandomForestRegressor parameters (max_depth, n_estimators, etc.)\n",
    "- Changing to a GradientBoostedRegressor model and adequately tuning the parameters.\n",
    "- Sourcing more data (this dataset only has 1000 records and approximately 300 have been removed due to NaN values)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b238f4",
   "metadata": {},
   "source": [
    "## Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93097bfe",
   "metadata": {},
   "source": [
    "#### Question 1 Response\n",
    "I sourced my code by referencing previously completed assignments as well as Lab 6 (Scaler, Pipelines, Gridsearch). I adapted and modified the code where applicable to suit my specific dataset and problem.\n",
    "\n",
    "#### Question 2 Response\n",
    "I completed the steps in the suggested order 1->2->3->4. I felt that this was a logical way to complete the assignment and was well laid out.\n",
    "\n",
    "#### Question 3 Response\n",
    "I used ChatGPT to help me find the name of the function astype() that I used in the data processing phase of assignment to \n",
    "convert the datatype of certain feature columns to integers so that they could be used in my regression model.\n",
    "\n",
    "#### Question 4 Response\n",
    "Yes; I ran into issues with my dataset because certain columns were being read in as objects/strings instead of integer datatypes. I eventually figured out that I needed to convert these to integers in the data processing step to make my models work properly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97b6ac",
   "metadata": {},
   "source": [
    "## Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challenging, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "I liked this assignment because it gave me the opportunity to put together what I have learned so far in the course and apply it to a dataset that I was interested in. I found the concept of Gridsearch a bit confusing at first but referencing the notes and related Lab exercise helped me proceed to complete the assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
